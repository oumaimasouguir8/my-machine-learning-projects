{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning with Scikit-Learn: Support Vector Machines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEP4oGn5P6HqRvwMj8dceh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oumaima61/my-machine-learning-projects/blob/master/Machine_Learning_with_Scikit_Learn_Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYy5ESJelM8j"
      },
      "source": [
        "This notebook will give an in-depth look at one of the most powerful of classical machine learning techniques: support vector machines (SVMs).\n",
        "\n",
        "In it, we will build intuition into the support vector machines algorithm from the ground up, including implementations in scikit-learn. I will introduce the idea of discriminative classifiers, and motivate SVMs as a maximum margin classifier. Finally, we will take a look at a kernelized SVM and softening of the margin in order to fit more complicated datasets.\n",
        "\n",
        "At the end of this notebook, you will:\n",
        "\n",
        "  Have a qualitative understanding of the Support Vector Machine model\n",
        "  Understand the concept of kernel methods in machine learning\n",
        "  Understand how support vector machines can be implemented and used in scikit-learn\n",
        "\n",
        "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will develop the intuition behind support vector machines and their use in classification problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAABTbE1k7s9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "plt.style.use('seaborn') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_uwk1_LldOS"
      },
      "source": [
        "Motivating Support Vector Machines "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPsys8Clk3x"
      },
      "source": [
        "n Machine Learning with Scikit-Learn: Introduction to Machine Learning, we made use of the LinearSVC classifier without much additional explanation. Here we will be taking a more in-depth look at support vector classifiers in general, and how they can be effectively used to model complex datasets.\n",
        "\n",
        "As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJHTSIck8AM"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9iVjBrJlv1C"
      },
      "source": [
        "One simple approach to classification is to try to draw a straight line between the two groups of points; this is known as a linear discriminative classifier. For two-dimensional data like that shown here, this is a task we could do by hand.\n",
        "\n",
        "But, we immediately see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PcP9YaGl0H9"
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5); "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXUQE5h6k8IT"
      },
      "source": [
        "## These are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point will be assigned a different label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMU6X17qk8MK"
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5); "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xv2i3t6k8Qo"
      },
      "source": [
        "##Support Vector Machines: Maximizing the Margin\n",
        "##Support vector machines offer one way to improve on this. \n",
        "##The idea is this: rather than simply drawing a zero-width line between the classes,\n",
        "##we can draw around each line a margin of some width, up to the nearest point.\n",
        " ##Here is an example of how this might look"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WAWPvJyk8UU"
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wywhaen8mdQF"
      },
      "source": [
        "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZAYsW3lml0s"
      },
      "source": [
        "Fitting a Support Vector Machine\n",
        "\n",
        "Let's see the result of an actual fit to this data: we will use scikit-learn's support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZuiDsRTmm4B"
      },
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UY9CVzzm5IU"
      },
      "source": [
        "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQlcsEJVk8Yd"
      },
      "source": [
        "from lesson4 import plot_svc_decision_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcVUnnnkk8b1"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Cv4RLjnNdV"
      },
      "source": [
        "This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. In scikit-learn, the identity of these points is stored in the support_vectors_ attribute of the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aefr88q9k8fP"
      },
      "source": [
        "model.support_vectors_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2iiqrw3nbF7"
      },
      "source": [
        "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin that are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
        "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "324E4EUDk8i3"
      },
      "source": [
        "from lesson4 import plot_svm\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsM4Gp8Vnmax"
      },
      "source": [
        "In the left panel, we see the model and the support vectors for 60 training points. In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59RcDAnUk8nN"
      },
      "source": [
        "from ipywidgets import interact, fixed\n",
        "interact(plot_svm, N=(10, 200), ax=fixed(None));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MIpynNEnvq9"
      },
      "source": [
        "Beyond Linear Boundaries: Kernel SVM\n",
        "\n",
        "SVM becomes extremely powerful when it is combined with kernels. To motivate the need for kernels, let's look at some data that is not linearly separable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6KbZkrSk8qi"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False); "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWyjq7EQn8bE"
      },
      "source": [
        "It is clear that no linear discrimination will ever be able to separate this data. But we can draw a lesson from the basis function regressions in Machine Learning with Scikit-Learn: Hyperparamters and Model Validation, and think about how we might project the data into a higher dimension such that a linear separator would be sufficient.\n",
        "\n",
        "For example, one simple projection we could use would be to compute a radial basis function centered on the middle clump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iZ4inxVn2tV"
      },
      "source": [
        "r = np.exp(-(X ** 2).sum(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_PpS2uonE3"
      },
      "source": [
        "We can visualize this extra data dimension using a three-dimensional plot—-if you are running this notebook live, you will be able to use the sliders to rotate the plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSwG4I30ojkk"
      },
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
        "    ax = plt.subplot(projection='3d')\n",
        "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('r')\n",
        "\n",
        "# Filter distracting warnings from ipywidgets\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "interact(plot_3D, elev=(0, 90), azip=(-180, 180),\n",
        "         X=fixed(X), y=fixed(y));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reLwuhR6owUQ"
      },
      "source": [
        "In the above plot, we can see that with this additional dimension, the data becomes trivially and linearly separable, by drawing a separating plane at, say, r=0.7.\n",
        "\n",
        "Here, we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
        "\n",
        "One strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
        "\n",
        "A potential problem with this strategy—projecting N points into N dimensions—is that it might become very computationally intensive as N grows large. However, because of a neat little procedure known as the kernel trick, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full N-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
        "\n",
        "In scikit-learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO7vvfqJosJ_"
      },
      "source": [
        "clf = SVC(kernel='rbf', C=1E6)\n",
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVWWrD3Vo15N"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9j8DeU9pXuU"
      },
      "source": [
        "Tuning the SVM: Softening Margins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvHBs_a4pfnT"
      },
      "source": [
        "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
        " But what if your data has some amount of overlap? \n",
        " For example, you may have data like this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MflLpFhvpSQI"
      },
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=1.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKwoFMAgptEf"
      },
      "source": [
        "To handle this case, the SVM implementation has a bit of a fudge factor that \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C. For very large C, the margin is hard and points cannot lie in it. For smaller C, the margin is softer and can grow to encompass some points.\n",
        "\n",
        "The next plot gives a visual of how a changing C parameter affects the final fit, via the softening of the margin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRqAvRvDpmtA"
      },
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.8)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1ltAC03qU1b"
      },
      "source": [
        "The optimal value of the C parameter will depend on your dataset, \n",
        "and should be tuned using cross-validation or a similar procedure \n",
        "(refer back to Machine Learning with Scikit-Learn: Hyperparameters and Model Validation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju1sJObNqUk1"
      },
      "source": [
        "Example of SVMs: Face Recognition¶\n",
        "\n",
        "As an example of support vector machines in action, let's take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into scikit-learn:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtTZfbpxqMIB"
      },
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfGcBzVIqswZ"
      },
      "source": [
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlnTN2ZDqv9A"
      },
      "source": [
        "### Let's plot a few of these faces to see what we're working with:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAbEzdMtq3ei"
      },
      "source": [
        "fig, ax = plt.subplots(3, 5)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[],\n",
        "            xlabel=faces.target_names[faces.target[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GTVxlPorDK6"
      },
      "source": [
        "Each image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but it is often more effective to use some sort of preprocessor to extract more meaningful features. Here we will use a dimensionality reduction technique known as Principal Component Analysis (see the Principal Component Analysis section of the Python Data Science Handbook for details) to extract 150 fundamental components to feed into our support vector machine classifier.\n",
        "\n",
        "We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeOpGcJbq64V"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca = PCA(n_components=150, svd_solver='randomized', whiten=True, random_state=42)\n",
        "svc = SVC(kernel='rbf', class_weight='balanced')\n",
        "model = make_pipeline(pca, svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDQI38g9rIAn"
      },
      "source": [
        "### For the sake of testing our classifier output, we will split the data into a training and testing set:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p08q9zG1rPjY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
        "                                                random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgixU8rerWh-"
      },
      "source": [
        "Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t67v1DyrTH5"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'svc__C': [1, 5, 10, 50],\n",
        "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
        "grid = GridSearchCV(model, param_grid)\n",
        "\n",
        "%time grid.fit(Xtrain, ytrain)\n",
        "print(grid.best_params_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8c59-QKr_nt"
      },
      "source": [
        "The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\n",
        "\n",
        "Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q31-o53rTFe"
      },
      "source": [
        "model = grid.best_estimator_\n",
        "yfit = model.predict(Xtest) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFinZP3psLjf"
      },
      "source": [
        "Let's take a look at a few of the test images along with their predicted values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXypaMq4rTDw"
      },
      "source": [
        "fig, ax = plt.subplots(4, 6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
        "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
        "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmOELGSssaTN"
      },
      "source": [
        "Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s\n",
        "face in the bottom row was mislabeled as Blair).\n",
        "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EMvB_bQsRFh"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, yfit,\n",
        "                            target_names=faces.target_names)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAn6uFugst0m"
      },
      "source": [
        "We might also display the confusion matrix between these classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nIQb0mdsjNG"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "mat = confusion_matrix(ytest, yfit)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cmap='Blues', cbar=False,\n",
        "            xticklabels=faces.target_names,\n",
        "            yticklabels=faces.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH1MA_hTtASe"
      },
      "source": [
        "**summmary** : \n",
        "\n",
        "This helps us get a sense of which labels are likely to be confused by the estimator.\n",
        "\n",
        "For a real-world facial recognition task, in which the photos do not come precropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces and extract features that are independent of the pixellation. For this kind of application, one good option is to make use of OpenCV, which, among other things, includes pretrained implementations of state-of-the-art feature extraction tools for images in general and faces in particular.\n"
      ]
    }
  ]
}