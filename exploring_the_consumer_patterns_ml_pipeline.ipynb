{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exploring-the-consumer-patterns-ml-pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqkj9dHUpOYfG5UB847mgp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oumaima61/my-machine-learning-projects/blob/master/exploring_the_consumer_patterns_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrB3oQQic0qQ"
      },
      "source": [
        "# Necessary librarys\n",
        "import os # it's a operational system library, to set some informations\n",
        "import random # random is to generate random values\n",
        "\n",
        "import pandas as pd # to manipulate data frames \n",
        "import numpy as np # to work with matrix\n",
        "from scipy.stats import kurtosis, skew # it's to explore some statistics of numerical values\n",
        "\n",
        "import matplotlib.pyplot as plt # to graphics plot\n",
        "import seaborn as sns # a good library to graphic plots\n",
        "import squarify # to better understand proportion of categorys - it's a treemap layout algorithm\n",
        "\n",
        "# Importing librarys to use on interactive graphs\n",
        "from plotly.offline import init_notebook_mode, iplot, plot \n",
        "import plotly.graph_objs as go \n",
        "\n",
        "import json # to convert json in df\n",
        "from pandas import json_normalize # to normalize the json file\n",
        "\n",
        "# to set a style to all graphs\n",
        "plt.style.use('fivethirtyeight')\n",
        "init_notebook_mode(connected=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY1zfgZK7pEl"
      },
      "source": [
        "Some Columns Are in JSON Format So It Will Be Necessary to Deal With This Problem.\n",
        "\n",
        "I will use a chunk of code that almost all kernels are using. I dont know who did first, but I found it on SRK kernel and I made some modifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCgHZ_5B7r3_"
      },
      "source": [
        "columns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n",
        "\n",
        "dir_path = \"\" # you can change to your local \n",
        "\n",
        "# p is a fractional number to skiprows and read just a random sample of the our dataset. \n",
        "p = 0.07 # *** In this case we will use 50% of data set *** #\n",
        "\n",
        "#Code to transform the json format columns in table\n",
        "def json_read(df):\n",
        "    #joining the [ path + df received]\n",
        "    data_frame = dir_path + df\n",
        "    \n",
        "    #Importing the dataset\n",
        "    df = pd.read_csv(data_frame, \n",
        "                     converters={column: json.loads for column in columns}, # loading the json columns properly\n",
        "                     dtype={'fullVisitorId': 'str'}, # transforming this column to string\n",
        "                     skiprows=lambda i: i>0 and random.random() > p)# Number of rows that will be imported randomly\n",
        "    \n",
        "    for column in columns: #loop to finally transform the columns in data frame\n",
        "        #It will normalize and set the json to a table\n",
        "        column_as_df = json_normalize(df[column]) \n",
        "        # here will be set the name using the category and subcategory of json columns\n",
        "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns] \n",
        "        # after extracting the values, let drop the original columns\n",
        "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
        "        \n",
        "    # Printing the shape of dataframes that was imported     \n",
        "    print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n",
        "    return df # returning the df after importing and transforming"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAC_14QR76Ys"
      },
      "source": [
        "Importing the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tsZbqZa70sH"
      },
      "source": [
        "%%time \n",
        "# %%time is used to calculate the timing of code chunk execution #\n",
        "\n",
        "# We will import the data using the name and extension that will be concatenated with dir_path\n",
        "df_train = json_read(\"template_bh/data/train.csv\") \n",
        "# The same to test dataset\n",
        "#df_test = json_read(\"test.csv\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA2PkoG58KPQ"
      },
      "source": [
        "Nice. After the import and transformation, we have 54 columns. Now, let's see our data and handle with problemns that we will find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ypSCIU70pN"
      },
      "source": [
        "# This command shows the first 5 rows of our dataset\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVsX2su-8Ulf"
      },
      "source": [
        "Knowing the Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGQ1Ufop8Pu-"
      },
      "source": [
        "# code chunk that I saw in Gabriel Preda kernel\n",
        "def missing_values(data):\n",
        "    total = data.isnull().sum().sort_values(ascending = False) # getting the sum of null values and ordering\n",
        "    percent = (data.isnull().sum() / data.isnull().count() * 100 ).sort_values(ascending = False) #getting the percent and order of null\n",
        "    df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) # Concatenating the total and percent\n",
        "    print(\"Total columns at least one Values: \")\n",
        "    print (df[~(df['Total'] == 0)]) # Returning values of nulls different of 0\n",
        "    \n",
        "    print(\"\\n Total of Sales % of Total: \", round((df_train[df_train['totals.transactionRevenue'] != np.nan]['totals.transactionRevenue'].count() / len(df_train['totals.transactionRevenue']) * 100),4))\n",
        "    \n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_9ZXpjs8Z51"
      },
      "source": [
        "# calling the missing values function\n",
        "missing_values(df_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCDlCWR88kAv"
      },
      "source": [
        "Let's Take a Look on Data Types of All Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr3QpAQV8ejd"
      },
      "source": [
        "print(df_train.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GwOAmBq8wag"
      },
      "source": [
        "Creating the Function to Handle with Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyX0LadK8p2h"
      },
      "source": [
        "# library of datetime\n",
        "from datetime import datetime\n",
        "\n",
        "# This function is to extract date features\n",
        "def date_process(df):\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\") # seting the column as pandas datetime\n",
        "    df[\"_weekday\"] = df['date'].dt.weekday #extracting week day\n",
        "    df[\"_day\"] = df['date'].dt.day # extracting day\n",
        "    df[\"_month\"] = df['date'].dt.month # extracting day\n",
        "    df[\"_year\"] = df['date'].dt.year # extracting day\n",
        "    df['_visitHour'] = (df['visitStartTime'].apply(lambda x: str(datetime.fromtimestamp(x).hour))).astype(int)\n",
        "    \n",
        "    return df #returning the df after the transformations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuQvOAqf82Mh"
      },
      "source": [
        "### Calling the function "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j385WQpv89--"
      },
      "source": [
        "df_train = date_process(df_train) #calling the function that we created above\n",
        "\n",
        "df_train.head(n=2) #printing the first 2 rows of our dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsjfEWTp9A-w"
      },
      "source": [
        "def FillingNaValues(df):    # fillna numeric feature\n",
        "    df['totals.pageviews'].fillna(1, inplace=True).astype(int) #filling NA's with 1\n",
        "    df['totals.newVisits'].fillna(0, inplace=True).astype(int) #filling NA's with 0\n",
        "    df['totals.bounces'].fillna(0, inplace=True).astype(int)   #filling NA's with 0\n",
        "    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].fillna(0.0).astype(float) #filling NA with zero\n",
        "    df['trafficSource.isTrueDirect'].fillna(False, inplace=True) # filling boolean with False\n",
        "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # filling boolean with True\n",
        "    df_train.loc[df_train['geoNetwork.city'] == \"(not set)\", 'geoNetwork.city'] = np.nan\n",
        "    df_train['geoNetwork.city'].fillna(\"NaN\", inplace=True)\n",
        "\n",
        "    return df #return the transformed dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR2K1BFP9H9K"
      },
      "source": [
        "def NumericalColumns(df):    # fillna numeric feature\n",
        "    df['totals.pageviews'].fillna(1, inplace=True) #filling NA's with 1\n",
        "    df['totals.newVisits'].fillna(0, inplace=True) #filling NA's with 0\n",
        "    df['totals.bounces'].fillna(0, inplace=True)   #filling NA's with 0\n",
        "    df['trafficSource.isTrueDirect'].fillna(False, inplace=True) # filling boolean with False\n",
        "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # filling boolean with True\n",
        "    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].fillna(0.0).astype(float) #filling NA with zero\n",
        "    df['totals.pageviews'] = df['totals.pageviews'].astype(int) # setting numerical column as integer\n",
        "    df['totals.newVisits'] = df['totals.newVisits'].astype(int) # setting numerical column as integer\n",
        "    df['totals.bounces'] = df['totals.bounces'].astype(int)  # setting numerical column as integer\n",
        "    df[\"totals.hits\"] = df[\"totals.hits\"].astype(float) # setting numerical to float\n",
        "    df['totals.visits'] = df['totals.visits'].astype(int) # seting as int\n",
        "\n",
        "    return df #return the transformed dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT_5QoeS9TVt"
      },
      "source": [
        "Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlgzyGZB9TFT"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def Normalizing(df):\n",
        "    # Use MinMaxScaler to normalize the column\n",
        "    df[\"totals.hits\"] =  (df['totals.hits'] - min(df['totals.hits'])) / (max(df['totals.hits'])  - min(df['totals.hits']))\n",
        "    # normalizing the transaction Revenue\n",
        "    df['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x: np.log1p(x))\n",
        "    # return the modified df\n",
        "    return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd-Qd6x09PYy"
      },
      "source": [
        "### Let's Investigate Some Constant Columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgMBde-m9dSI"
      },
      "source": [
        "# We will takeoff all columns where we have a unique value (constants)\n",
        "# It is useful because this columns don't give us none information\n",
        "discovering_consts = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
        "\n",
        "# printing the total of columns dropped and the name of columns \n",
        "print(\"Columns with just one value: \", len(discovering_consts), \"columns\")\n",
        "print(\"Name of constant columns: \\n\", discovering_consts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac3oK8uW9jn2"
      },
      "source": [
        "#Here are all columns that the unique value is 'not available in demo dataset'\n",
        "\n",
        "not_aval_cols = ['socialEngagementType','device.browserSize','device.browserVersion', 'device.flashVersion', \n",
        "                 'device.language' ,'device.mobileDeviceBranding', 'device.mobileDeviceInfo','device.mobileDeviceMarketingName',\n",
        "                 'device.mobileDeviceModel', 'device.mobileInputSelector' , 'device.operatingSystemVersion','device.screenColors',\n",
        "                 'device.screenResolution', 'geoNetwork.cityId', 'geoNetwork.latitude' ,'geoNetwork.longitude',\n",
        "                 'geoNetwork.networkLocation','trafficSource.adwordsClickInfo.criteriaParameters']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk5CnIdx9uDS"
      },
      "source": [
        "It's Useful To Know That We Have 23 Constant Columns\n",
        " Below, I will set a function to better investigate our data and correctly categorize them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQS_rF9-9pra"
      },
      "source": [
        "# seting the function to show \n",
        "def knowningData(df, data_type=object, limit=3): #seting the function with df, \n",
        "    n = df.select_dtypes(include=data_type) #selecting the desired data type\n",
        "    for column in n.columns: #initializing the loop\n",
        "        print(\"##############################################\")\n",
        "        print(\"Name of column \", column, ': \\n', \"Uniques: \", df[column].unique()[:limit], \"\\n\",\n",
        "              \" | ## Total nulls: \", (round(df[column].isnull().sum() / len(df[column]) * 100,2)),\n",
        "              \" | ## Total unique values: \", df_train.nunique()[column]) #print the data and % of nulls)\n",
        "        # print(\"Percentual of top 3 of: \", column)\n",
        "        # print(round(df[column].value_counts()[:3] / df[column].value_counts().sum() * 100,2))\n",
        "        print(\"#############################################\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxXIZFvs-BgC"
      },
      "source": [
        "I Will Categorize by Object Data Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWKTAXJ694up"
      },
      "source": [
        "# calling our function: object is default\n",
        "knowningData(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6524tO94-VBn"
      },
      "source": [
        "Printing Integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx7Qp-GK-Fxi"
      },
      "source": [
        "knowningData(df_train, data_type=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h3YtY7X-dUW"
      },
      "source": [
        "Printing Float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuajp13_-Z-U"
      },
      "source": [
        "knowningData(df_train, data_type=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml4JcPDR-k5A"
      },
      "source": [
        "I Will Drop Some of These Features and Fill N/A or Missing in Some of Them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjqvTM6y-hfD"
      },
      "source": [
        "to_drop = [\"socialEngagementType\",'device.browserVersion', 'device.browserSize', 'device.flashVersion', 'device.language', \n",
        "           'device.mobileDeviceBranding', 'device.mobileDeviceInfo', 'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',\n",
        "           'device.mobileInputSelector', 'device.operatingSystemVersion', 'device.screenColors', 'device.screenResolution', \n",
        "           'geoNetwork.cityId', 'geoNetwork.latitude', 'geoNetwork.longitude','geoNetwork.networkLocation', \n",
        "           'trafficSource.adwordsClickInfo.criteriaParameters', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.campaign',\n",
        "           'trafficSource.adwordsClickInfo.page', 'trafficSource.referralPath', 'trafficSource.adwordsClickInfo.slot',\n",
        "           'trafficSource.adContent', 'trafficSource.keyword']\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf1L_00R-qT9"
      },
      "source": [
        "df_train.drop(to_drop, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUd0JETU-tw5"
      },
      "source": [
        "print(\"Total features dropped: \", len(to_drop))\n",
        "print(\"Shape after dropping: \", df_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhn0lYb9-tjL"
      },
      "source": [
        "# call the function to transform the numerical columns\n",
        "df_train = NumericalColumns(df_train)\n",
        "\n",
        "# Call the function that will normalize some features\n",
        "df_train = Normalizing(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OdZn-1y_M1d"
      },
      "source": [
        "Checking for Mistakes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9siQNxQp_UZE"
      },
      "source": [
        "Let's See the Unique Values in Our Dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHKNu7w-tYh"
      },
      "source": [
        "# We will takeoff all columns where we have a unique value\n",
        "# It is useful because this columns don't give us none information\n",
        "clean_consts = [col for col in df_train.columns if df_train[col].nunique() == 1]\n",
        "\n",
        "\n",
        "# this function drop all constant columns, inplacing the data \n",
        "df_train.drop('trafficSource.adwordsClickInfo.adNetworkType', axis=1, inplace=True) \n",
        "\n",
        "# printing the total of columns dropped and the name of columns \n",
        "print(\"This useful action will drop: \", len(clean_consts), \"columns\")\n",
        "print(\"All dropped columns: \\n\", clean_consts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtRilbq8_nk9"
      },
      "source": [
        "The output show us `totals.visits` and `trafficSource.adwordsClickInfo.adNetworkType`, but `totals.visits` can be useful, so I will drop just `trafficSource` feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbQaunMB_bSf"
      },
      "source": [
        "df_train.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llrBwj2Y_18i"
      },
      "source": [
        " Based on This Output, I Will Select and Set a Variable with All Features by Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43PlLFk_wVU"
      },
      "source": [
        "'trafficSource.adwordsClickInfo.adNetworkType'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93VQu-KJD85S"
      },
      "source": [
        "dummy_feaures =['channelGrouping', 'device.browser', 'device.deviceCategory', 'geoNetwork.city', 'device.operatingSystem', \n",
        "                'trafficSource.medium', 'trafficSource.source',\n",
        "                'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', \n",
        "                'geoNetwork.subContinent']\n",
        "\n",
        "\n",
        "numericals = ['totals.visits', '_visitHour', '_day', '_month', '_weekday']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7OTbzPcED2A"
      },
      "source": [
        "First, Let's See the Distribuition of Transactions Revenues¶\n",
        "\n",
        "I will start exploring the quantile.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TXK1ocDEAGa"
      },
      "source": [
        "# Printing some statistics of our data\n",
        "print(\"Transaction Revenue Min Value: \", \n",
        "      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].min()) # printing the min value\n",
        "print(\"Transaction Revenue Mean Value: \", \n",
        "      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].mean()) # mean value\n",
        "print(\"Transaction Revenue Median Value: \", \n",
        "      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].median()) # median value\n",
        "print(\"Transaction Revenue Max Value: \", \n",
        "      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].max()) # the max value\n",
        "\n",
        "# It I did to plot the quantiles but are not working\n",
        "#print(round(df_train['totals.transactionRevenue'].quantile([.025,.25,.5,.75,.975]),2))\n",
        "\n",
        "# seting the figure size of our plots\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "# Subplot allow us to plot more than one \n",
        "# in this case, will be create a subplot grid of 2 x 1\n",
        "plt.subplot(1,2,1)\n",
        "# seting the distribuition of our data and normalizing using np.log on values highest than 0 and + \n",
        "# also, we will set the number of bins and if we want or not kde on our histogram\n",
        "ax = sns.distplot(np.log(df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"] + 0.01), bins=40, kde=True)\n",
        "ax.set_xlabel('Transaction RevenueLog', fontsize=15) #seting the xlabel and size of font\n",
        "ax.set_ylabel('Distribuition', fontsize=15) #seting the ylabel and size of font\n",
        "ax.set_title(\"Distribuition of Revenue Log\", fontsize=20) #seting the title and size of font\n",
        "\n",
        "# setting the second plot of our grid of graphs\n",
        "plt.subplot(1,2,2)\n",
        "# ordering the total of users and seting the values of transactions to understanding \n",
        "plt.scatter(range(df_train.shape[0]), np.sort(df_train['totals.transactionRevenue'].values))\n",
        "plt.xlabel('Index', fontsize=15) # xlabel and size of words\n",
        "plt.ylabel('Revenue value', fontsize=15) # ylabel and size of words\n",
        "plt.title(\"Revenue Value Distribution\", fontsize=20) # Setting Title and fontsize\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-S7VrpbERO0"
      },
      "source": [
        "Kurtosis and Skewness of Transaction Revenue\n",
        "\n",
        "Skew and Kurtosis are two important statistics terms you need to know.\n",
        "Skewness\n",
        "\n",
        "Skewness is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\n",
        "\n",
        "It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n",
        "\n",
        "Positive Skewness means the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.\n",
        "\n",
        "Negative Skewness is when the tail of the left side of the distribution is longer or fatter than the tail on the right side. The mean and median will be less than the mode.\n",
        "So, When Is the Skewness Too Much?\n",
        "\n",
        "The rule of thumb seems to be:\n",
        " If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n",
        " If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n",
        "If the skewness is less than -1 (negatively skewed) or greater than 1 (positively skewed), the data are highly skewed.\n",
        "\n",
        "Kurtosis\n",
        "\n",
        "Kurtosis is all about the tails of the distributio—not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail.\n",
        "\n",
        "It is actually the measure of outliers present in the distribution.\n",
        "\n",
        "High kurtosis in a data set is an indicator that data has heavy tails or outliers. If there is a high kurtosis, then we need to investigate why we have so many outliers. It indicates a lot of things; perhaps incorrect data entry or other things. Investigate!\n",
        "\n",
        "Low kurtosis in a data set is an indicator that data has light tails or lack of outliers. If we get low kurtosis (too good to be true), then we also need to investigate and trim the dataset of unwanted results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuEXbQATEKI8"
      },
      "source": [
        "print('Excess kurtosis of normal distribution (should be 0): {}'.format(\n",
        "    kurtosis(df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"])))\n",
        "print( 'Skewness of normal distribution (should be 0): {}'.format(\n",
        "    skew((df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIo8y5acEgm5"
      },
      "source": [
        "Function That I Created to Find the Map Outlier Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utz9NirQEbo9"
      },
      "source": [
        "def CalcOutliers(df_num): \n",
        "    '''\n",
        "    \n",
        "    Leonardo Ferreira 20/10/2018\n",
        "    Set a numerical value and it will calculate the upper, lower and total number of outliers\n",
        "    It will print a lot of statistics of the numerical feature that you set on input\n",
        "    \n",
        "    '''\n",
        "    # calculating mean and std of the array\n",
        "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
        "\n",
        "    # seting the cut line to both higher and lower values\n",
        "    # You can change this value\n",
        "    cut = data_std * 3\n",
        "\n",
        "    #Calculating the higher and lower cut values\n",
        "    lower, upper = data_mean - cut, data_mean + cut\n",
        "\n",
        "    # creating an array of lower, higher and total outlier values \n",
        "    outliers_lower = [x for x in df_num if x < lower]\n",
        "    outliers_higher = [x for x in df_num if x > upper]\n",
        "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
        "\n",
        "    # array without outlier values\n",
        "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
        "    \n",
        "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
        "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
        "    print('Identified outliers: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
        "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
        "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pTGe_TvEy3V"
      },
      "source": [
        "CalcOutliers(df_train['totals.transactionRevenue']) # Call "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqDaOXuVE3Pk"
      },
      "source": [
        "CalcOutliers(df_train['totals.pageviews']) # Call "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98kRcvC_E_oP"
      },
      "source": [
        "Device Browsers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FoyFe46E8SX"
      },
      "source": [
        "# the top 10 of browsers represent % of total\n",
        "print(\"Percentual of Browser usage: \")\n",
        "print(df_train['device.browser'].value_counts()[:7] ) # printing the top 7 percentage of browsers\n",
        "\n",
        "# seting the graph size\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "# Let explore the browser used by users\n",
        "sns.countplot(df_train[df_train['device.browser']\\\n",
        "                       .isin(df_train['device.browser']\\\n",
        "                             .value_counts()[:10].index.values)]['device.browser'], palette=\"hls\") # It's a module to count the category's\n",
        "plt.title(\"TOP 10 Most Frequent Browsers\", fontsize=20) # Adding Title and seting the size\n",
        "plt.xlabel(\"Browser Names\", fontsize=16) # Adding x label and seting the size\n",
        "plt.ylabel(\"Count\", fontsize=16) # Adding y label and seting the size\n",
        "plt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n",
        "\n",
        "plt.show() #use plt.show to render the graph that we did above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkeCWf40FgRV"
      },
      "source": [
        "What If We Cross the Revenue and Browser?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gBFFr2nFb_F"
      },
      "source": [
        "plt.figure(figsize=(13,6)) #figure size\n",
        "\n",
        "#It's another way to plot our data. using a variable that contains the plot parameters\n",
        "g1 = sns.boxenplot(x='device.browser', y='totals.transactionRevenue', \n",
        "                   data=df_train[(df_train['device.browser'].isin((df_train['device.browser'].value_counts()[:10].index.values))) &\n",
        "                                  df_train['totals.transactionRevenue'] > 0])\n",
        "g1.set_title('Browsers Name by Transactions Revenue', fontsize=20) # title and fontsize\n",
        "g1.set_xticklabels(g1.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\n",
        "g1.set_xlabel('Device Names', fontsize=18) # Xlabel\n",
        "g1.set_ylabel('Trans Revenue(log) Dist', fontsize=18) #Ylabel\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whOg6T2oFuCW"
      },
      "source": [
        "Let's See the Channel Grouping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLs829DJFppE"
      },
      "source": [
        "# the top 10 of browsers represent % of total\n",
        "print(\"Percentual of Channel Grouping used: \")\n",
        "print((df_train['channelGrouping'].value_counts()[:5])) # printing the top 7 percentage of browsers\n",
        "\n",
        "# seting the graph size\n",
        "plt.figure(figsize=(14,7))\n",
        "\n",
        "# let explore the browser used by users\n",
        "sns.countplot(df_train[\"channelGrouping\"], palette=\"hls\") # It's a module to count the category's\n",
        "plt.title(\"Channel Grouping Count\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"Channel Grouping Name\", fontsize=18) # seting the x label size\n",
        "plt.ylabel(\"Count\", fontsize=18) # seting the y label size\n",
        "\n",
        "plt.show() #use plt.show to render the graph that we did above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW0HB2S9FzLY"
      },
      "source": [
        "### Crossing Channel Grouping x Browsers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjyC8UYbF5-2"
      },
      "source": [
        "## I will use the crosstab to explore two categorical values\n",
        "\n",
        "# At index I will use set my variable that I want analyse and cross by another\n",
        "crosstab_eda = pd.crosstab(index=df_train['channelGrouping'], normalize=True,\n",
        "                           # at this line, I am using the isin to select just the top 5 of browsers\n",
        "                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n",
        "                                                                            .value_counts()[:5].index.values)]['device.browser'])\n",
        "# Ploting the crosstab that we did above\n",
        "crosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n",
        "                 figsize=(14,7), # adjusting the size of graphs\n",
        "                 stacked=True)   # code to unstack \n",
        "plt.title(\"Channel Grouping % for which Browser\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"The Channel Grouping Name\", fontsize=18) # seting the x label size\n",
        "plt.ylabel(\"Count\", fontsize=18) # seting the y label size\n",
        "plt.xticks(rotation=0)\n",
        "plt.show() # rendering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYOX4O5BGaB8"
      },
      "source": [
        "Operational System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGf784YhF-La"
      },
      "source": [
        "# the top 5 of browsers represent % of total\n",
        "print(\"Percentual of Operational System: \")\n",
        "print(df_train['device.operatingSystem'].value_counts()[:5]) # printing the top 7 percentage of browsers\n",
        "\n",
        "# seting the graph size\n",
        "plt.figure(figsize=(14,7))\n",
        "\n",
        "# let explore the browser used by users\n",
        "sns.countplot(df_train[\"device.operatingSystem\"], palette=\"hls\") # It's a module to count the category's\n",
        "plt.title(\"Operational System used Count\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"Operational System Name\", fontsize=16) # seting the x label size\n",
        "plt.ylabel(\"OS Count\", fontsize=16) # seting the y label size\n",
        "plt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n",
        "\n",
        "plt.show() #use plt.show to render the graph that we did above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxEJ0tTaGlqb"
      },
      "source": [
        "Now Let's Investigate the Most Used Brower by Operational System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uhOlGEPGhoJ"
      },
      "source": [
        "# At index I will use isin to substitute the loop and get just the values with more than 1%\n",
        "crosstab_eda = pd.crosstab(index=df_train[df_train['device.operatingSystem']\\\n",
        "                                          .isin(df_train['device.operatingSystem']\\\n",
        "                                                .value_counts()[:6].index.values)]['device.operatingSystem'], \n",
        "                           \n",
        "                           # at this line, I am using the isin to select just the top 5 of browsers\n",
        "                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n",
        "                                                                            .value_counts()[:5].index.values)]['device.browser'])\n",
        "# Ploting the crosstab that we did above\n",
        "crosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n",
        "                 figsize=(14,7), # adjusting the size of graphs\n",
        "                 stacked=True)   # code to unstack \n",
        "plt.title(\"Most frequent OS's by Browsers of users\", fontsize=22) # adjusting title and fontsize\n",
        "plt.xlabel(\"Operational System Name\", fontsize=19) # adjusting x label and fontsize\n",
        "plt.ylabel(\"Count OS\", fontsize=19) # adjusting y label and fontsize\n",
        "plt.xticks(rotation=0) # Adjust the xticks, rotating the labels\n",
        "\n",
        "plt.show() # rendering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v9LEpYZGv1y"
      },
      "source": [
        "I Will Explore the Distribution of Transaction Revenue by Each OS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1bV3tPcGrWf"
      },
      "source": [
        "(sns.FacetGrid(df_train[(df_train['device.operatingSystem']\\\n",
        "                        .isin(df_train['device.operatingSystem']\\\n",
        "                              .value_counts()[:6].index.values)) & df_train['totals.transactionRevenue'] > 0],\n",
        "               hue='device.operatingSystem', height=5, aspect=2)\n",
        "  .map(sns.kdeplot, 'totals.transactionRevenue', shade=True)\n",
        " .add_legend()\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhRt-8LjG4ll"
      },
      "source": [
        "Let's Investigate the Device Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhtJigsgG04J"
      },
      "source": [
        "# the top 5 of browsers represent % of total\n",
        "print(\"Percentual of Operational System: \")\n",
        "print(round(df_train['device.deviceCategory'].value_counts() / len(df_train['device.deviceCategory']) * 100, 2)) # printing the top 7 percentage of browsers\n",
        "\n",
        "# seting the graph size\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "# let explore the browser used by users\n",
        "sns.countplot(df_train[\"device.deviceCategory\"], palette=\"hls\") # It's a module to count the category's\n",
        "plt.title(\"Device Category Count\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"Device Category\", fontsize=18) # seting the x label size\n",
        "plt.ylabel(\"Count\", fontsize=16) # seting the y label size\n",
        "plt.xticks(fontsize=18) # Adjust the xticks, rotating the labels\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.boxenplot(x=\"device.deviceCategory\", y = 'totals.transactionRevenue', \n",
        "              data=df_train[df_train['totals.transactionRevenue'] > 0], palette=\"hls\") # It's a module to count the category's\n",
        "plt.title(\"Device Category Revenue Distribuition\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"Device Category\", fontsize=18) # seting the x label size\n",
        "plt.ylabel(\"Revenue(Log)\", fontsize=16) # seting the y label size\n",
        "plt.xticks(fontsize=18) # Adjust the xticks, rotating the labels\n",
        "\n",
        "plt.subplots_adjust(hspace = 0.9, wspace = 0.5)\n",
        "\n",
        "plt.show() #use plt.show to render the graph that we did above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oU9vUyfHBPt"
      },
      "source": [
        "Let's See the Difference Distribution Between Devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmuYl8s1G9k1"
      },
      "source": [
        "(sns.FacetGrid(df_train[df_train['totals.transactionRevenue'] > 0],\n",
        "               hue='device.deviceCategory', height=5, aspect=2)\n",
        "  .map(sns.kdeplot, 'totals.transactionRevenue', shade=True)\n",
        " .add_legend()\n",
        ")\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqt4sJtlHGMV"
      },
      "source": [
        "Now, Let's Investigate the Device Category by Browsers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlEy00DnHLAL"
      },
      "source": [
        "# At index I will use isin to substitute the loop and get just the values with more than 1%\n",
        "crosstab_eda = pd.crosstab(index=df_train['device.deviceCategory'], # at this line, I am using the isin to select just the top 5 of browsers\n",
        "                           columns=df_train[df_train['device.operatingSystem']\\\n",
        "                                            .isin(df_train['device.operatingSystem']\\\n",
        "                                                  .value_counts()[:6].index.values)]['device.operatingSystem'])\n",
        "# Ploting the crosstab that we did above\n",
        "crosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n",
        "                 figsize=(14,7), # adjusting the size of graphs\n",
        "                 stacked=True)   # code to unstack \n",
        "plt.title(\"Most frequent OS's by Device Categorys of users\", fontsize=22) # adjusting title and fontsize\n",
        "plt.xlabel(\"Device Name\", fontsize=19)                # adjusting x label and fontsize\n",
        "plt.ylabel(\"Count Device x OS\", fontsize=19)                               # adjusting y label and fontsize\n",
        "plt.xticks(rotation=0)                                            # Adjust the xticks, rotating the labels\n",
        "\n",
        "\n",
        "plt.show() # rendering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QefTFtNbHPUj"
      },
      "source": [
        "# the top 8 of browsers represent % of total \n",
        "print(\"Description of SubContinent count: \")\n",
        "print(df_train['geoNetwork.subContinent'].value_counts()[:8]) # printing the top 7 percentage of browsers\n",
        "\n",
        "# seting the graph size\n",
        "plt.figure(figsize=(16,7))\n",
        "\n",
        "# let explore the browser used by users\n",
        "sns.countplot(df_train[df_train['geoNetwork.subContinent']\\\n",
        "                       .isin(df_train['geoNetwork.subContinent']\\\n",
        "                             .value_counts()[:15].index.values)]['geoNetwork.subContinent'], palette=\"hls\") # It's a module to count the category's\n",
        "plt.title(\"TOP 15 most frequent SubContinents\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"subContinent Names\", fontsize=18) # seting the x label size\n",
        "plt.ylabel(\"SubContinent Count\", fontsize=18) # seting the y label size\n",
        "plt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n",
        "\n",
        "plt.show() #use plt.show to render the graph that we did above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6B-VEC5HVVj"
      },
      "source": [
        "## I will use the crosstab to explore two categorical values\n",
        "\n",
        "# At index I will use isin to substitute the loop and get just the values with more than 1%\n",
        "crosstab_eda = pd.crosstab(index=df_train[df_train['geoNetwork.subContinent']\\\n",
        "                                          .isin(df_train['geoNetwork.subContinent']\\\n",
        "                                                .value_counts()[:10].index.values)]['geoNetwork.subContinent'], \n",
        "                           \n",
        "                           # at this line, I am using the isin to select just the top 5 of browsers\n",
        "                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n",
        "                                                                            .value_counts()[:5].index.values)]['device.browser'])\n",
        "# Ploting the crosstab that we did above\n",
        "crosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n",
        "                 figsize=(16,7), # adjusting the size of graphs\n",
        "                 stacked=True)   # code to unstack \n",
        "plt.title(\"TOP 10 Most frequent Subcontinents by Browsers used\", fontsize=22) # adjusting title and fontsize\n",
        "plt.xlabel(\"Subcontinent Name\", fontsize=19) # adjusting x label and fontsize\n",
        "plt.ylabel(\"Count Subcontinent\", fontsize=19) # adjusting y label and fontsize\n",
        "plt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n",
        "plt.legend(loc=1, prop={'size': 12}) # to \n",
        "\n",
        "plt.show() # rendering "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg3rMmYWHePX"
      },
      "source": [
        "print('train date:', min(df_train['date']), 'to', max(df_train['date']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAb2XJ8bHeMn"
      },
      "source": [
        "year = df_train['_year'].value_counts()         # counting the Year with value counts\n",
        "month = df_train['_month'].value_counts()      # coutning months\n",
        "weeday = df_train['_weekday'].value_counts()    # Couting weekday\n",
        "day = df_train['_day'].value_counts()              # counting Day\n",
        "date = df_train['date'].value_counts()           # Counting date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynyVqJbpHo3H"
      },
      "source": [
        "Interactive Date Features\n",
        "First I Will Explore Revenue and Number of Visits By Day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVM0ABh6HeKg"
      },
      "source": [
        "# I saw and take a lot of inspiration to this interactive plots in kernel: \n",
        "# https://www.kaggle.com/jsaguiar/complete-exploratory-analysis-all-columns\n",
        "# I learned a lot in this kernel and I will implement and adapted some ideas\n",
        "\n",
        "#seting some static color options\n",
        "color_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n",
        "            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n",
        "            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n",
        "\n",
        "# Visits by time train\n",
        "\n",
        "# couting all entries by date to get number of visits by each date\n",
        "dates_temp = df_train['date'].value_counts().to_frame().reset_index().sort_values('index') \n",
        "# renaming the columns to apropriate names\n",
        "dates_temp = dates_temp.rename(columns = {\"date\" : \"visits\"}).rename(columns = {\"index\" : \"date\"})\n",
        "\n",
        "# creating the first trace with the necessary parameters\n",
        "trace = go.Scatter(x=dates_temp.date.astype(str), y=dates_temp.visits,\n",
        "                    opacity = 0.8, line = dict(color = color_op[3]), name= 'Visits by day')\n",
        "\n",
        "# Below we will get the total values by Transaction Revenue Log by date\n",
        "dates_temp_sum = df_train.groupby('date')['totals.transactionRevenue'].sum().to_frame().reset_index()\n",
        "\n",
        "# using the new dates_temp_sum we will create the second trace\n",
        "trace1 = go.Scatter(x=dates_temp_sum.date.astype(str), line = dict(color = color_op[1]), name=\"RevenueLog by day\",\n",
        "                        y=dates_temp_sum['totals.transactionRevenue'], opacity = 0.8)\n",
        "\n",
        "# Getting the total values by Transactions by each date\n",
        "dates_temp_count = df_train[df_train['totals.transactionRevenue'] > 0].groupby('date')['totals.transactionRevenue'].count().to_frame().reset_index()\n",
        "\n",
        "# using the new dates_temp_count we will create the third trace\n",
        "trace2 = go.Scatter(x=dates_temp_count.date.astype(str), line = dict(color = color_op[5]), name=\"Sellings by day\",\n",
        "                        y=dates_temp_count['totals.transactionRevenue'], opacity = 0.8)\n",
        "\n",
        "#creating the layout the will allow us to give an title and \n",
        "# give us some interesting options to handle with the outputs of graphs\n",
        "layout = dict(\n",
        "    title= \"Informations by Date\",\n",
        "    xaxis=dict(\n",
        "        rangeselector=dict(\n",
        "            buttons=list([\n",
        "                dict(count=1, label='1m', step='month', stepmode='backward'),\n",
        "                dict(count=3, label='3m', step='month', stepmode='backward'),\n",
        "                dict(count=6, label='6m', step='month', stepmode='backward'),\n",
        "                dict(step='all')\n",
        "            ])\n",
        "        ),\n",
        "        rangeslider=dict(visible = True),\n",
        "        type='date'\n",
        "    )\n",
        ")\n",
        "\n",
        "# creating figure with the both traces and layout\n",
        "fig = dict(data= [trace, trace1, trace2], layout=layout)\n",
        "\n",
        "#rendering the graphs\n",
        "iplot(fig) #it's an equivalent to plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPUp_unaHu8a"
      },
      "source": [
        "## Creating a Sophistcated Interactive graphic to Better Understand Date Features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rprvaDPuH7nE"
      },
      "source": [
        "Select the Option"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SB9Yc6rIDgp"
      },
      "source": [
        "# Setting the first trace\n",
        "trace1 = go.Histogram(x=df_train[\"_year\"],\n",
        "                      name='Year Count')\n",
        "\n",
        "# Setting the second trace\n",
        "trace2 = go.Histogram(x=df_train[\"_month\"],\n",
        "                name='Month Count')\n",
        "\n",
        "# Setting the third trace\n",
        "trace3 = go.Bar(y=day.values,\n",
        "                x=day.index.values, \n",
        "                name='Day Count')\n",
        "\n",
        "# Setting the fourth trace\n",
        "trace4 = go.Bar(y=weeday.values,\n",
        "                x=weeday.index.values,\n",
        "                name='Weekday Count')\n",
        "\n",
        "# puting all traces in the same \"array of graphics\" to we render it below\n",
        "data = [trace1, trace2, trace4, trace3]\n",
        "\n",
        "#Creating the options to be posible we use in our \n",
        "updatemenus = list([\n",
        "    dict(active=-1,\n",
        "         x=-0.15,\n",
        "         buttons=list([  \n",
        "             dict(\n",
        "                 label = 'Years Count',\n",
        "                 method = 'update',\n",
        "                 args = [{'visible': [True, False, False, False,False]}, \n",
        "                         {'title': 'Count of Year'}]),\n",
        "             dict(\n",
        "                 label = 'Months Count',\n",
        "                 method = 'update',\n",
        "                 args = [{'visible': [False, True, False, False,False]},\n",
        "                         {'title': 'Count of Months'}]),\n",
        "             dict(\n",
        "                 label = 'WeekDays Count',\n",
        "                 method = 'update',\n",
        "                 args = [{'visible': [False, False, True, False, False]},\n",
        "                         {'title': 'Count of WeekDays'}]),\n",
        "            dict(\n",
        "                label = 'Days Count ',\n",
        "                method = 'update',\n",
        "                args = [{'visible': [False, False, False, True,False]},\n",
        "                        {'title': 'Count of Day'}]) ])\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "layout = dict(title='The percentual Distribuitions of Date Features (Select from Dropdown)',\n",
        "              showlegend=False,\n",
        "              updatemenus=updatemenus,\n",
        "#              xaxis = dict(\n",
        "#                  type=\"category\"\n",
        "#                      ),\n",
        "              barmode=\"group\"\n",
        "             )\n",
        "fig = dict(data=data, layout=layout)\n",
        "print(\"SELECT BELOW: \")\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tioiBsecH7P5"
      },
      "source": [
        "## Very Cool graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vDii6W5IN6S"
      },
      "source": [
        "Let's investigate the VisitHour and weekday to see if we can find some interesting patterns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Y6EHFxH4uO"
      },
      "source": [
        "date_sales = ['_visitHour', '_weekday'] #seting the desired \n",
        "\n",
        "cm = sns.light_palette(\"green\", as_cmap=True)\n",
        "pd.crosstab(df_train[date_sales[0]], df_train[date_sales[1]], \n",
        "            values=df_train[\"totals.transactionRevenue\"], aggfunc=[np.sum]).style.background_gradient(cmap = cm)\n",
        "\n",
        "# tab.columns.levels[1] = [\"Sun\", \"Mon\", \"Thu\", \"wed\", \"Thi\",\"Fri\",\"Sat\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjkeZBC-IXcw"
      },
      "source": [
        "I Will Use an Interesting Graphic Called Squarify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUzHM16-ISiQ"
      },
      "source": [
        "number_of_colors = 20 # total number of different collors that we will use\n",
        "\n",
        "# Here I will generate a bunch of hexadecimal colors \n",
        "color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
        "             for i in range(number_of_colors)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBTiBfguIiNi"
      },
      "source": [
        "Exploring Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boaldOQ3Id0O"
      },
      "source": [
        "country_tree = df_train[\"geoNetwork.country\"].value_counts() #counting the values of Country\n",
        "\n",
        "print(\"Description most frequent countrys: \")\n",
        "print(country_tree[:15]) #printing the 15 top most \n",
        "\n",
        "country_tree = round((df_train[\"geoNetwork.country\"].value_counts()[:30] \\\n",
        "                       / len(df_train['geoNetwork.country']) * 100),2)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "g = squarify.plot(sizes=country_tree.values, label=country_tree.index, \n",
        "                  value=country_tree.values,\n",
        "                  alpha=.4, color=color)\n",
        "g.set_title(\"'TOP 30 Countrys - % size of total\",fontsize=20)\n",
        "g.set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JdLcMbcIuFo"
      },
      "source": [
        " Now, I Will Look on the City Feature and See the Principal Cities in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDu2SB9OIorJ"
      },
      "source": [
        "df_train.loc[df_train[\"geoNetwork.city\"] == \"not available in demo dataset\", 'geoNetwork.city'] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMGW3QFdIylG"
      },
      "source": [
        "city_tree = df_train[\"geoNetwork.city\"].value_counts() #counting \n",
        "\n",
        "print(\"Description most frequent Citys: \" )\n",
        "print(city_tree[:15])\n",
        "\n",
        "city_tree = round((city_tree[:30] / len(df_train['geoNetwork.city']) * 100),2)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "g = squarify.plot(sizes=city_tree.values, label=city_tree.index, \n",
        "                  value=city_tree.values,\n",
        "                  alpha=.4, color=color)\n",
        "g.set_title(\"'TOP 30 Citys - % size of total\",fontsize=20)\n",
        "g.set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMxwCb6zI24k"
      },
      "source": [
        "## Creating a Function with plotly to Better Investigate the Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2mZAW2kI8_3"
      },
      "source": [
        "def PieChart(df_colum, title, limit=15):\n",
        "    \"\"\"\n",
        "    This function helps to investigate the proportion of visits and total of transction revenue \n",
        "    by each category\n",
        "    \"\"\"\n",
        "\n",
        "    count_trace = df_train[df_colum].value_counts()[:limit].to_frame().reset_index()\n",
        "    rev_trace = df_train.groupby(df_colum)[\"totals.transactionRevenue\"].sum().nlargest(10).to_frame().reset_index()\n",
        "\n",
        "    trace1 = go.Pie(labels=count_trace['index'], values=count_trace[df_colum], name= \"% Acesses\", hole= .5, \n",
        "                    hoverinfo=\"label+percent+name\", showlegend=True,domain= {'x': [0, .48]}, \n",
        "                    marker=dict(colors=color))\n",
        "\n",
        "    trace2 = go.Pie(labels=rev_trace[df_colum], \n",
        "                    values=rev_trace['totals.transactionRevenue'], name=\"% Revenue\", hole= .5, \n",
        "                    hoverinfo=\"label+percent+name\", showlegend=False, domain= {'x': [.52, 1]})\n",
        "\n",
        "    layout = dict(title= title, height=450, font=dict(size=15),\n",
        "                  annotations = [\n",
        "                      dict(\n",
        "                          x=.25, y=.5,\n",
        "                          text='Visits', \n",
        "                          showarrow=False,\n",
        "                          font=dict(size=20)\n",
        "                      ),\n",
        "                      dict(\n",
        "                          x=.80, y=.5,\n",
        "                          text='Revenue', \n",
        "                          showarrow=False,\n",
        "                          font=dict(size=20)\n",
        "                      )\n",
        "        ])\n",
        "\n",
        "    fig = dict(data=[trace1, trace2], layout=layout)\n",
        "    iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHMURlI8JFVQ"
      },
      "source": [
        "Device Category feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yPqotw7JBgn"
      },
      "source": [
        "PieChart(\"device.deviceCategory\", \"Device Category\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MNXmrv7JKYK"
      },
      "source": [
        "## I Will Apply the Pie Chart on Countries Again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYhqmEJ7JPHq"
      },
      "source": [
        "# call the function\n",
        "PieChart(\"geoNetwork.city\", \"Top Cities by Accesses and Revenue\", limit=12) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE1SlAVFJXEa"
      },
      "source": [
        "Seeing Again Channel Grouping More Specified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhQgVMZkJSxs"
      },
      "source": [
        "PieChart(\"channelGrouping\", \"Channel Grouping Visits and Revenues\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TPlGe-rJfhE"
      },
      "source": [
        "Let's See the NetWork Domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTpFEZr3Jbo7"
      },
      "source": [
        "PieChart('geoNetwork.networkDomain', \"Network Domain\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ855lNnJki6"
      },
      "source": [
        "PieChart(\"device.deviceCategory\", \"Device Category\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENhalAx_JssX"
      },
      "source": [
        "Trafic Source Medium "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0_APQjNJorQ"
      },
      "source": [
        "PieChart(\"trafficSource.medium\", \"Trafic Source - Medium\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLF0blZKJxbQ"
      },
      "source": [
        "PieChart('trafficSource.source', \"Visits and Revenue by TOP Sources\", limit=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Oy98RAJ7iU"
      },
      "source": [
        "I Will Continue This Notebook! Vote Up the Kernel and Stay Tuned for the Next Updates "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqWnZ9FkJxZJ"
      },
      "source": [
        "df_train.corr()['totals.transactionRevenue']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_dQ_tpgJxUK"
      },
      "source": [
        "country_repayment = ['channelGrouping', '_weekday'] #seting the desired \n",
        "\n",
        "cm = sns.light_palette(\"green\", as_cmap=True)\n",
        "pd.crosstab(df_train[country_repayment[0]], df_train[country_repayment[1]], \n",
        "            values=df_train[\"totals.transactionRevenue\"], aggfunc=[np.sum]).style.background_gradient(cmap = cm)\n",
        "\n",
        "# tab.columns.levels[1] = [\"Sun\", \"Mon\", \"Thu\", \"wed\", \"Thi\",\"Fri\",\"Sat\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PaUGu3-KLZG"
      },
      "source": [
        "Geolocation Plot to Visually Understand the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSRjzEyzKGVb"
      },
      "source": [
        "# Counting total visits by countrys\n",
        "countMaps = pd.DataFrame(df_train['geoNetwork.country'].value_counts()).reset_index()\n",
        "countMaps.columns=['country', 'counts'] #renaming columns\n",
        "countMaps = countMaps.reset_index().drop('index', axis=1) #reseting index and droping the column\n",
        "\n",
        "data = [ dict(\n",
        "        type = 'choropleth',\n",
        "        locations = countMaps['country'],\n",
        "        locationmode = 'country names',\n",
        "        z = countMaps['counts'],\n",
        "        text = countMaps['country'],\n",
        "        autocolorscale = False,\n",
        "        marker = dict(\n",
        "            line = dict (\n",
        "                color = 'rgb(180,180,180)',\n",
        "                width = 0.5\n",
        "            ) ),\n",
        "        colorbar = dict(\n",
        "            autotick = False,\n",
        "            tickprefix = '',\n",
        "            title = 'Number of Visits'),\n",
        "      ) ]\n",
        "\n",
        "layout = dict(\n",
        "    title = 'Couting Visits Per Country',\n",
        "    geo = dict(\n",
        "        showframe = False,\n",
        "        showcoastlines = True,\n",
        "        projection = dict(\n",
        "            type = 'Mercator'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "figure = dict( data=data, layout=layout )\n",
        "iplot(figure, validate=False, filename='map-countrys-count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbWVpIhqKT3T"
      },
      "source": [
        "Total Revenue by Country "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF2OIgEqKQZ2"
      },
      "source": [
        "# I will crete a variable of Revenues by country sum\n",
        "sumRevMaps = df_train[df_train['totals.transactionRevenue'] > 0].groupby(\"geoNetwork.country\")[\"totals.transactionRevenue\"].count().to_frame().reset_index()\n",
        "sumRevMaps.columns = [\"country\", \"count_sales\"] # renaming columns\n",
        "sumRevMaps = sumRevMaps.reset_index().drop('index', axis=1) #reseting index and drop index column\n",
        "\n",
        "data = [ dict(\n",
        "        type = 'choropleth',\n",
        "        locations = sumRevMaps['country'],\n",
        "        locationmode = 'country names',\n",
        "        z = sumRevMaps['count_sales'],\n",
        "        text = sumRevMaps['country'],\n",
        "        autocolorscale = False,\n",
        "        marker = dict(\n",
        "            line = dict (\n",
        "                color = 'rgb(180,180,180)',\n",
        "                width = 0.5\n",
        "            ) ),\n",
        "        colorbar = dict(\n",
        "            autotick = False,\n",
        "            tickprefix = '',\n",
        "            title = 'Count of Sales'),\n",
        "      ) ]\n",
        "\n",
        "layout = dict(\n",
        "    title = 'Total Sales by Country',\n",
        "    geo = dict(\n",
        "        showframe = False,\n",
        "        showcoastlines = True,\n",
        "        projection = dict(\n",
        "            type = 'Mercator'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "figure = dict( data=data, layout=layout )\n",
        "\n",
        "iplot(figure, validate=False, filename='map-countrys-total')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejyKDQN1KZQc"
      },
      "source": [
        "## Some Tests That I Am Doing to Try Find Interesting Feature Engineering Approaches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy8hZUacKfFe"
      },
      "source": [
        "df_train['month_unique_user_count'] = df_train.groupby('_month')['fullVisitorId'].transform('nunique')\n",
        "df_train['day_unique_user_count'] = df_train.groupby('_day')['fullVisitorId'].transform('nunique')\n",
        "df_train['weekday_unique_user_count'] = df_train.groupby('_weekday')['fullVisitorId'].transform('nunique')\n",
        "\n",
        "\n",
        "df_train['traf_sourc_browser_count'] = df_train.groupby(['trafficSource.medium', 'device.browser'])['totals.pageviews'].transform('nunique')\n",
        "df_train['Id_browser_pageviews_sumprod'] = df_train.groupby(['fullVisitorId', 'device.browser'])['totals.pageviews'].transform('cumprod')\n",
        "df_train['Id_browser_hits_sumprod'] = df_train.groupby(['fullVisitorId', 'device.browser'])['totals.hits'].transform('cumprod')\n",
        "df_train['Id_browser_hits_sumprod'] = df_train.groupby(['fullVisitorId', 'device.browser'])['totals.hits'].transform('cumprod')\n",
        "df_train['Id_browser_hits_sumprod_mob'] = df_train.groupby(['fullVisitorId', 'device.browser', 'device.isMobile'])['totals.hits'].transform('sum')\n",
        "\n",
        "df_train['Id_networkDomain_hits'] = df_train.groupby(['fullVisitorId', 'geoNetwork.networkDomain'])['totals.hits'].transform('var')\n",
        "# df_train['Id_networkDomain_country_hits'] = df_train.groupby(['fullVisitorId', 'geoNetwork.networkDomain', 'geoNetwork.country'])['totals.hits'].transform(lambda x: x.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWLn3pP0KkSb"
      },
      "source": [
        "# df_train[[\"totals.transactionRevenue\", 'Id_browser_hits_sumprod', 'Id_networkDomain_hits','Id_networkDomain_country_hits', 'Id_browser_hits_sumprod_mob']].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlfmkcSjKq0w"
      },
      "source": [
        "## Preprocessing the Full Dataset and Creating New Features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVi8qc9VKqyU"
      },
      "source": [
        "aggs = {\n",
        "    'date': ['min', 'max'],\n",
        "    'totals.hits': ['sum', 'min', 'max', 'mean', 'median'],\n",
        "    'totals.pageviews': ['sum', 'min', 'max', 'mean', 'median'],\n",
        "    'totals.bounces': ['sum', 'mean', 'median'],\n",
        "    'totals.newVisits': ['sum', 'mean', 'median']\n",
        "}\n",
        "\n",
        "# Previous applications categorical features\n",
        "cat_aggregations = {}\n",
        "\n",
        "for cat in dummy_feaures:\n",
        "    cat_aggregations[cat] = ['min', 'max', 'mean']\n",
        "\n",
        "prev_agg = df_train.groupby('fullVisitorId').agg({**aggs})\n",
        "\n",
        "prev_agg.columns = pd.Index(['Agg_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04anCaqIKqvs"
      },
      "source": [
        "prev_agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ClXrEhWK5NQ"
      },
      "source": [
        "new_columns = [\n",
        "        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
        "    ]\n",
        "new_columns "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF-RF6guK_Di"
      },
      "source": [
        "dummy_feaures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV3p0dp6LDle"
      },
      "source": [
        "### Testing some grouping approaches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIXst5-LGWg"
      },
      "source": [
        "df_train['cumcount'] = df_train.groupby('fullVisitorId').cumcount() + 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5uEhiXLK4O"
      },
      "source": [
        "## Some tests to feature engineering "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k_LxZFULY0b"
      },
      "source": [
        "aggs = {\n",
        "    'date': ['min', 'max'],\n",
        "    'totals.transactionRevenue': ['sum', 'size'],\n",
        "    'totals.hits': ['sum', 'min', 'max', 'count', 'median'],\n",
        "    'totals.pageviews': ['sum', 'min', 'max', 'mean', 'median'],\n",
        "    'totals.bounces': ['sum', 'mean', 'median'],\n",
        "    'totals.newVisits': ['sum', 'mean', 'median']\n",
        "}\n",
        "\n",
        "# Previous applications categorical features\n",
        "cat_aggregations = {}\n",
        "\n",
        "for cat in dummy_feaures:\n",
        "    cat_aggregations[cat] = ['min', 'max', 'mean']\n",
        "\n",
        "prev_agg = df_train.groupby('fullVisitorId').agg({**aggs})\n",
        "\n",
        "prev_agg.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65YlKXnvLeBf"
      },
      "source": [
        "I Will Continue Working on This Kernel, Stay Tuned\n",
        "\n",
        "Please, if you liked this kernel don't forget to votes up and give your feedback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91QIIzxLLldR"
      },
      "source": [
        "prev_agg.columns = [\"_\".join(x) for x in prev_agg.columns.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbPTeh7ZLo9J"
      },
      "source": [
        "prev_agg.head() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}