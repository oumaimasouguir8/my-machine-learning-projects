{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGnzw98VDax/i3SY2RIB9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oumaima61/my-machine-learning-projects/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2zKuNle8rZc",
        "outputId": "d68ac208-ac8b-490e-be64-e4f6b499432e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentence = \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\"\n",
        "\n",
        "sentence_tokenized = nltk.word_tokenize(sentence)\n",
        "\n",
        "print (sentence_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy3qc5Md9518",
        "outputId": "2fbd4ae5-7eff-401f-b07e-5655f3a620cc"
      },
      "source": [
        "paragraph = \"\"\"\n",
        "            Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, \n",
        "            but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”\n",
        "\"\"\"\n",
        "\n",
        "para_tokenized = nltk.word_tokenize(paragraph)\n",
        "\n",
        "print (para_tokenized) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', '“', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', '”', 'thought', 'Alice', '“', 'without', 'pictures', 'or', 'conversations', '?', '”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asQkIPl9-UXF",
        "outputId": "8743510f-3ed5-4233-a94e-54fc36ac204d"
      },
      "source": [
        "paragraph = \"\"\"\n",
        "            As she said this she looked down at her hands, and was surprised to \n",
        "            see that she had put on one of the Rabbit’s little white kid gloves while she was talking. “How can I have done that?” she thought. “I must be growing small again.”\n",
        "\"\"\"\n",
        "\n",
        "para_tokenized = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "print (para_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n            As she said this she looked down at her hands, and was surprised to \\n            see that she had put on one of the Rabbit’s little white kid gloves while she was talking.', '“How can I have done that?” she thought.', '“I must be growing small again.”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCwuFmDf-fe9"
      },
      "source": [
        "Normalisation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRE21MA5-bsR",
        "outputId": "8f1b3ae4-df55-4cb8-da65-3e138f8625aa"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "#Download stop words from nltk library\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = nltk.word_tokenize(paragraph)\n",
        "\n",
        "clean_paragraph = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "try:\n",
        "  print (\"print tokens \")\n",
        "  print (clean_paragraph)\n",
        "except:\n",
        "  print (\"expected variable called clean_sentence\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "print tokens \n",
            "['As', 'said', 'looked', 'hands', ',', 'surprised', 'see', 'put', 'one', 'Rabbit', '’', 'little', 'white', 'kid', 'gloves', 'talking', '.', '“', 'How', 'I', 'done', '?', '”', 'thought', '.', '“', 'I', 'must', 'growing', 'small', '.', '”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE2x4fMp-xeO",
        "outputId": "00dd4881-4767-43cd-867b-51b024b3c60c"
      },
      "source": [
        "sentence = \"Wow this is cool ! wow this is awesome.\"\n",
        "lit_sentence = sentence.lower()\n",
        "upp_sentence = sentence.upper()\n",
        "\n",
        "print (lit_sentence)\n",
        "\n",
        "print (upp_sentence) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wow this is cool ! wow this is awesome.\n",
            "WOW THIS IS COOL ! WOW THIS IS AWESOME.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0SZsdtT-80k"
      },
      "source": [
        " Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OJ0MBQP-3vS",
        "outputId": "db6fec61-87fe-4a3f-8e05-a6f91ffbc9dd"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"There might be some sense in your knocking.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "word_stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print (\"List of Tokens \")\n",
        "print (tokens)\n",
        "\n",
        "print (\"\\n\")\n",
        "print (\"List of stemmed words\")\n",
        "print (word_stemmed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of Tokens \n",
            "['There', 'might', 'be', 'some', 'sense', 'in', 'your', 'knocking', '.']\n",
            "\n",
            "\n",
            "List of stemmed words\n",
            "['there', 'might', 'be', 'some', 'sens', 'in', 'your', 'knock', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "498xMuRw_LlR",
        "outputId": "02909828-f590-458d-98bd-b3f251e4b5b5"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "sentence = \"There might be some sense in your knocking.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemma = [Lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print (lemma) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['There', 'might', 'be', 'some', 'sense', 'in', 'your', 'knocking', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRMniHDX_aGb"
      },
      "source": [
        "Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOFTimI9_TIV"
      },
      "source": [
        "corpus = [\"Emma was a Catholic because her mother was a Catholic, and Nory’s mother was a Catholic because her father was a Catholic, and her father was a Catholic because his mother was a Catholic, or had been.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xMCd_Cb_kY5",
        "outputId": "1911bded-a08b-4d7e-d91f-11d68dea7b0b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus_vectorizer = CountVectorizer()\n",
        "corpus_vectorizer.fit(corpus) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKR-seNz_r_r",
        "outputId": "e0d5e629-5e70-41ae-8705-2287522d1fd0"
      },
      "source": [
        "#show count for each word present in corpus\n",
        "corpus_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'because': 1,\n",
              " 'been': 2,\n",
              " 'catholic': 3,\n",
              " 'emma': 4,\n",
              " 'father': 5,\n",
              " 'had': 6,\n",
              " 'her': 7,\n",
              " 'his': 8,\n",
              " 'mother': 9,\n",
              " 'nory': 10,\n",
              " 'or': 11,\n",
              " 'was': 12}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRHkVCIq_yUj"
      },
      "source": [
        "corpus = \"Emma was a Catholic because her mother was a Catholic, and Nory’s mother was a Catholic because her father was a Catholic, and her father was a Catholic because his mother was a Catholic, or had been.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im0APUIb_5gi",
        "outputId": "4b0952d7-c028-40ae-c319-131fcc6ce60f"
      },
      "source": [
        "#Python code without any nlp library\n",
        "tokens = corpus.split()\n",
        "#Uncomment to see all the tokens\n",
        "#print (tokens)\n",
        "#define empty dictionary \n",
        "dic = {}\n",
        "\n",
        "#save frequency of each token in dictiory\n",
        "for token in tokens:\n",
        "  count = 1\n",
        "  if token not in dic:\n",
        "    dic[token] = count\n",
        "  else:\n",
        "    dic[token] = dic[token] + 1\n",
        "\n",
        "print (dic) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Emma': 1, 'was': 6, 'a': 6, 'Catholic': 3, 'because': 3, 'her': 3, 'mother': 3, 'Catholic,': 3, 'and': 2, 'Nory’s': 1, 'father': 2, 'his': 1, 'or': 1, 'had': 1, 'been.': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrzB9opmABwm"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O29Nz7pHAcDZ",
        "outputId": "a598d730-4c6a-48c8-e436-f7176b8d5b14"
      },
      "source": [
        "doc_1 = \"She is cute\"\n",
        "doc_2 = \"He is very cute\"\n",
        "doc_3 = \"Angelina is very very cute\"\n",
        "\n",
        "corpus = [doc_1, doc_2, doc_3]\n",
        "\n",
        "corpus_preprocess = []\n",
        "\n",
        "for doc in corpus:\n",
        "  corpus_preprocess.append(doc.lower())\n",
        "\n",
        "corpus_vectorizer = TfidfVectorizer(norm=None)\n",
        "tf_idf_scores = corpus_vectorizer.fit_transform(corpus_preprocess)\n",
        "\n",
        "feature_names = corpus_vectorizer.get_feature_names()\n",
        "corpus_index = [doc for doc in corpus_preprocess]\n",
        "\n",
        "print (\"TF-IDF table of corpus\")\n",
        "print (\"======================\\n\")\n",
        "\n",
        "print(pd.DataFrame(tf_idf_scores.T.todense(), index = feature_names, columns = [\"doc_1\", \"doc_2\", \"doc_3\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF table of corpus\n",
            "======================\n",
            "\n",
            "             doc_1     doc_2     doc_3\n",
            "angelina  0.000000  0.000000  1.693147\n",
            "cute      1.000000  1.000000  1.000000\n",
            "he        0.000000  1.693147  0.000000\n",
            "is        1.000000  1.000000  1.000000\n",
            "she       1.693147  0.000000  0.000000\n",
            "very      0.000000  1.287682  2.575364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blp2QdPqAo00"
      },
      "source": [
        "Internally, `CountVectorizer` and `TfidfTransformer` are used to calculate term frequency and inverse document frequency in `TfidfVectorizer`. For a better understanding, a breakdown is given below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGzqYXqlAb-B",
        "outputId": "c83f1316-53ae-49e5-f6fe-675c51e4fcd7"
      },
      "source": [
        "#Part 1 - Calculating Term Frequencies\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "term_frequency = vectorizer.fit_transform(corpus_preprocess)\n",
        "\n",
        "#uncomment following to check feature names\n",
        "#print(vectorizer.get_feature_names())\n",
        "\n",
        "print (\"Term Frequency Matrix\")\n",
        "print (\"=====================\")\n",
        "print (term_frequency.toarray())\n",
        "print (\"=====================\")\n",
        "#Part 2 - Calculating Inverse Document Frequency\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "vectorizer = TfidfTransformer(norm=None)\n",
        "vectorizer.fit(term_frequency)\n",
        "\n",
        "idf = vectorizer.idf_\n",
        "\n",
        "df = pd.DataFrame(idf, index = feature_names, columns=['IDF'])\n",
        "print (\"\\nIDF table\")\n",
        "print (\"=========\")\n",
        "print(df)\n",
        "\n",
        "\n",
        "tf_idf_scores = vectorizer.fit_transform(term_frequency.toarray())\n",
        "\n",
        "\n",
        "print (\"\\nTF-IDF matrix\")\n",
        "print (\"=============\\n\")\n",
        "print (tf_idf_scores.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Term Frequency Matrix\n",
            "=====================\n",
            "[[0 1 0 1 1 0]\n",
            " [0 1 1 1 0 1]\n",
            " [1 1 0 1 0 2]]\n",
            "=====================\n",
            "\n",
            "IDF table\n",
            "=========\n",
            "               IDF\n",
            "angelina  1.693147\n",
            "cute      1.000000\n",
            "he        1.693147\n",
            "is        1.000000\n",
            "she       1.693147\n",
            "very      1.287682\n",
            "\n",
            "TF-IDF matrix\n",
            "=============\n",
            "\n",
            "[[0.         1.         0.         1.         1.69314718 0.        ]\n",
            " [0.         1.         1.69314718 1.         0.         1.28768207]\n",
            " [1.69314718 1.         0.         1.         0.         2.57536414]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}